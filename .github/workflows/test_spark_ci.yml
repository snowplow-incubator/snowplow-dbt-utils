name: Spark Deployment

on:
  pull_request:
    branches: [main]
  push:
    branches:
      - 'feature/**'
      - 'dev'
      - 'staging'

env:
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  AWS_REGION: eu-west-1
  AWS_DEFAULT_REGION: eu-west-1

jobs:
  deploy-spark:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v2

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v1

    - name: Install Docker Compose
      run: |
        sudo curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
        sudo chmod +x /usr/local/bin/docker-compose
        docker-compose --version

    - name: Update Dockerfile
      run: |
        cat > Dockerfile << EOL
        FROM openjdk:11-jre-slim

        # Set environment variables
        ENV SPARK_VERSION=3.5.1
        ENV HADOOP_VERSION=3.3.4
        ENV ICEBERG_VERSION=1.4.2
        ENV AWS_SDK_VERSION=1.12.581

        # Install necessary tools
        RUN apt-get update && apt-get install -y curl wget procps rsync ssh iputils-ping

        # Download and install Spark
        RUN wget --tries=5 --retry-connrefused --waitretry=1 --timeout=20 https://archive.apache.org/dist/spark/spark-\${SPARK_VERSION}/spark-\${SPARK_VERSION}-bin-hadoop3.tgz || \
            (echo "Failed to download Spark. Retrying with alternative mirror..." && \
             wget --tries=5 --retry-connrefused --waitretry=1 --timeout=20 https://downloads.apache.org/spark/spark-\${SPARK_VERSION}/spark-\${SPARK_VERSION}-bin-hadoop3.tgz) && \
            tar -xvzf spark-\${SPARK_VERSION}-bin-hadoop3.tgz && \
            mv spark-\${SPARK_VERSION}-bin-hadoop3 /spark && \
            rm spark-\${SPARK_VERSION}-bin-hadoop3.tgz

        # Set Spark environment variables
        ENV SPARK_HOME=/spark
        ENV PATH=\$PATH:\$SPARK_HOME/bin:\$SPARK_HOME/sbin

        # Download necessary JARs
        RUN mkdir -p /spark/jars && \
            wget https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/\${ICEBERG_VERSION}/iceberg-spark-runtime-3.5_2.12-\${ICEBERG_VERSION}.jar -O /spark/jars/iceberg-spark-runtime.jar && \
            wget https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-aws-bundle/\${ICEBERG_VERSION}/iceberg-aws-bundle-\${ICEBERG_VERSION}.jar -O /spark/jars/iceberg-aws-bundle.jar && \
            wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/\${HADOOP_VERSION}/hadoop-aws-\${HADOOP_VERSION}.jar -O /spark/jars/hadoop-aws.jar && \
            wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/\${AWS_SDK_VERSION}/aws-java-sdk-bundle-\${AWS_SDK_VERSION}.jar -O /spark/jars/aws-java-sdk-bundle.jar

        # Create directory for Spark events
        RUN mkdir -p /tmp/spark-events

        WORKDIR /spark

        CMD ["bash"]
        EOL

    - name: Build and start Spark cluster
      run: |
        docker-compose build --no-cache
        docker-compose up -d

    - name: Wait for services to start
      run: |
        echo "Waiting for Spark services to start..."
        sleep 60

    - name: Run network diagnostics
      run: |
        docker-compose exec -T spark-master /bin/bash -c "hostname && hostname -I"
        docker-compose exec -T spark-worker /bin/bash -c "hostname && hostname -I"
        docker-compose exec -T thrift-server /bin/bash -c "hostname && hostname -I"

    - name: Run test Spark job
      run: |
        echo "Running Spark Pi example job..."
        docker-compose exec -T spark-master /spark/bin/spark-submit \
          --class org.apache.spark.examples.SparkPi \
          --master spark://spark-master:7077 \
          --deploy-mode client \
          /spark/examples/jars/spark-examples*.jar 10
        echo "Spark job completed."

    - name: Verify Spark Thrift Server
      run: |
        echo "Verifying Spark Thrift Server..."
        docker-compose exec -T thrift-server /spark/bin/beeline -u "jdbc:hive2://localhost:10000" -e "SHOW DATABASES; SHOW TABLES IN default;"
        echo "Thrift Server verification completed."

    - name: Cleanup
      if: always()
      run: docker-compose down