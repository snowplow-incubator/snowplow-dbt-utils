name: Spark Deployment

on:
  pull_request:
    branches: [main]
  push:
    branches:
      - 'feature/**'
      - 'dev'
      - 'staging'

env:
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  AWS_REGION: eu-west-1
  AWS_DEFAULT_REGION: eu-west-1

jobs:
  deploy-spark:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v2

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v1

    - name: Install Docker Compose
      run: |
        sudo curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
        sudo chmod +x /usr/local/bin/docker-compose
        docker-compose --version

    - name: Update Dockerfile
      run: |
        sed -i 's|RUN wget https://downloads.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz|RUN wget --tries=5 --retry-connrefused --waitretry=1 --timeout=20 https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz || \\\n    (echo "Failed to download Spark. Retrying with alternative mirror..." && \\\n     wget --tries=5 --retry-connrefused --waitretry=1 --timeout=20 https://downloads.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz)|g' Dockerfile

    - name: Build and start Spark cluster
      run: |
        docker-compose build --no-cache
        docker-compose up -d

    - name: Wait for services to start
      run: |
        echo "Waiting for Spark services to start..."
        sleep 60

    - name: Run network diagnostics
      run: |
        docker-compose exec -T spark-master /bin/bash -c "hostname && hostname -I && ping -c 4 spark-master"
        docker-compose exec -T spark-worker /bin/bash -c "hostname && hostname -I && ping -c 4 spark-master"
        docker-compose exec -T thrift-server /bin/bash -c "hostname && hostname -I && ping -c 4 spark-master"

    - name: Run test Spark job
      run: |
        docker-compose exec -T spark-master /spark/bin/spark-submit --class org.apache.spark.examples.SparkPi \
          --master spark://spark-master:7077 \
          --deploy-mode client \
          /spark/examples/jars/spark-examples*.jar 10

    - name: Verify Spark Thrift Server
      run: |
        docker-compose exec -T thrift-server /spark/bin/beeline -u "jdbc:hive2://localhost:10000" -e "SHOW DATABASES; SHOW TABLES IN default;"

    - name: Cleanup
      if: always()
      run: docker-compose down